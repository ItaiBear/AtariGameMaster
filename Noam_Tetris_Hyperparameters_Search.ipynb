{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "74u8GL2dfD2F"
      },
      "outputs": [],
      "source": [
        "!rm -rf runs videos\n",
        "!rm db.sqlite3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KbrN9Li5i8"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSm_UBPsLzq_",
        "outputId": "39d8ba22-4808-4519-d1ce-388cb794e306"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install pyglet==1.5.1\n",
        "!pip install torchsummary\n",
        "!pip install optuna\n",
        "!pip install optuna-dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbigCoubxceQ",
        "outputId": "da980382-8858-44f8-e876-39872e99ac1f"
      },
      "outputs": [],
      "source": [
        "!pip install setuptools==65.5.1\n",
        "!pip install gym==0.21.0\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc0SC4CrMXSk",
        "outputId": "72a96155-d3d3-4f64-e143-6913e97a2d07"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay\n",
        "# !sudo apt-get install -y xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1NHYcVNMQaW",
        "outputId": "48e3c777-fc26-4b63-eba1-6e11e47aa30c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f33c074c910>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1024, 768))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the absolute path to the parent directory of gym-tetris\n",
        "gym_tetris_parent_path = os.path.abspath(os.path.join('..', 'gym-tetris'))\n",
        "\n",
        "# Append the path to the sys.path\n",
        "sys.path.append(gym_tetris_parent_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kImBr6NFjKIY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TidRAkGEU0Wl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.1.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# from stable_baselines3.common.atari_wrappers import (\n",
        "#     ClipRewardEnv,\n",
        "#     EpisodicLifeEnv,\n",
        "#     FireResetEnv,\n",
        "#     MaxAndSkipEnv,\n",
        "#     NoopResetEnv,\n",
        "# )\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_tetris\n",
        "from gym_tetris.actions import MOVEMENT\n",
        "\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import importlib\n",
        "# importlib.reload(gym_tetris)\n",
        "# importlib.reload(gym_tetris.actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H-rvKojYAuK3"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, actions_num, arch_fn):\n",
        "        super().__init__()\n",
        "        self.network = arch_fn(actions_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x / 255.0)\n",
        "\n",
        "def original(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, 8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, 4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, actions_num),\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def tiny(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        nn.Conv2d(1, 8, 3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(8, 16, 5, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 5, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(2048, 128),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, actions_num),\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def small(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        # (1, 84, 84)\n",
        "        nn.Conv2d(1, 64, 7, stride=3),\n",
        "        nn.ReLU(),\n",
        "        # (64, 26, 26)\n",
        "        nn.Conv2d(64, 128, 5),\n",
        "        nn.ReLU(),\n",
        "        # (128, 22, 22)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (128, 11, 11)\n",
        "        nn.Conv2d(128, 128, 3),\n",
        "        nn.ReLU(),\n",
        "        # (128, 8, 8)\n",
        "        nn.Conv2d(128, 256, 3),\n",
        "        nn.ReLU(),\n",
        "        # (256, 6, 6)\n",
        "        nn.Conv2d(256, 256, 3),\n",
        "        nn.ReLU(),\n",
        "        # (256, 4, 4)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (256, 2, 2)\n",
        "        nn.Flatten(),\n",
        "        # 1024\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(256, actions_num)\n",
        "        # 12 possible actions\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def large(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        # (1, 84, 84)\n",
        "        nn.Conv2d(1, 64, 7, stride=3),\n",
        "        nn.ReLU(),\n",
        "        # (128, 26, 26)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (128, 13, 13)\n",
        "        nn.Conv2d(64, 128, 4),\n",
        "        nn.ReLU(),\n",
        "        # (128, 10, 10)\n",
        "        nn.Conv2d(128, 256, 3),\n",
        "        nn.ReLU(),\n",
        "        # (256, 8, 8)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (256, 4, 4)\n",
        "        nn.Flatten(),\n",
        "        # 4096\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(512, actions_num),\n",
        "        # 12 possible actions\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def atari(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        # (1, 84, 84)\n",
        "        nn.Conv2d(1, 16, 8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        # (16, 20, 20)\n",
        "        nn.Conv2d(16, 32, 4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        # (32, 9, 9)\n",
        "        nn.Flatten(),\n",
        "        # 2592\n",
        "        nn.Linear(2592, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, actions_num),\n",
        "    )\n",
        "    return network\n",
        "\n",
        "\n",
        "\n",
        "def get_model_class(architecture=\"original\"):\n",
        "    if architecture == \"original\":\n",
        "        class OriginalQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, original)\n",
        "        return OriginalQNetwork\n",
        "    elif architecture == \"tiny\":\n",
        "        class TinyQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, tiny)\n",
        "        return TinyQNetwork\n",
        "    elif architecture == \"small\":\n",
        "        class SmallQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, small)\n",
        "        return SmallQNetwork\n",
        "    elif architecture == \"large\":\n",
        "        class LargeQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, large)\n",
        "        return LargeQNetwork\n",
        "    elif architecture == \"atari\":\n",
        "        class AtariQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, atari)\n",
        "        return AtariQNetwork\n",
        "    else:\n",
        "        print(\"Not a valid architecture\")\n",
        "\n",
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esiYvzSJEQrm"
      },
      "outputs": [],
      "source": [
        "# Network = get_model_class(\"small\")\n",
        "# model = Network(12)\n",
        "# summary(model, (1, 84, 84), batch_size=16, device=\"cpu\")\n",
        "# x = torch.randn(1, 84, 84)\n",
        "# out = model(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8KbPbg5G8_sO"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "I0yXM_e1TGkf"
      },
      "outputs": [],
      "source": [
        "def train(args, trial=None):\n",
        "  try:\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    prefix = \"\"\n",
        "    if trial:\n",
        "      run_name += f\"_trial_{trial.number}\"\n",
        "      prefix = f\"trial {trial.number}: \"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device_name = \"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\"\n",
        "    device_name = \"mps\" if torch.backends.mps.is_available() and args.mps else device_name\n",
        "    device = torch.device(device_name)\n",
        "\n",
        "    # env setup\n",
        "    env_fns = [make_env(args.env_id, args.seed, i, args.capture_video, run_name, args.video_frequency) for i in range(args.env_cnt)]\n",
        "    envs = gym.vector.SyncVectorEnv(env_fns)\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    q_network = args.QNetwork(envs.single_action_space.n).to(device)\n",
        "    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
        "    target_network = args.QNetwork(envs.single_action_space.n).to(device)\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    # summary(q_network, input_size=(1, 84, 84), batch_size=args.batch_size, device=device_name)\n",
        "\n",
        "    rb = ReplayBuffer(\n",
        "        args.buffer_size,\n",
        "        envs.single_observation_space,\n",
        "        envs.single_action_space,\n",
        "        device,\n",
        "        n_envs=args.env_cnt,\n",
        "        optimize_memory_usage=False,\n",
        "        handle_timeout_termination=True,\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    eval_idx = 1\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    obs = envs.reset()\n",
        "    for global_step in range(args.total_timesteps):\n",
        "        # ALGO LOGIC: put action logic here\n",
        "        epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
        "        if random.random() < epsilon:\n",
        "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "        else:\n",
        "            q_values = q_network(torch.Tensor(obs).to(device))\n",
        "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "\n",
        "        # TRY NOT TO MODIFY: execute the game and log data.\n",
        "        next_obs, rewards, dones, infos = envs.step(actions)\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        for info in infos:\n",
        "            if \"episode\" in info.keys():\n",
        "                print(f\"{prefix}global_step={global_step}, episodic_return={info['episode']['r']}, score={info['score']}\")\n",
        "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
        "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
        "                writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
        "                writer.add_scalar(\"charts/score\", info[\"score\"], global_step)\n",
        "\n",
        "\n",
        "            # Evaluate and report the agent periodically\n",
        "            if trial and global_step % args.eval_frequency == 0 and global_step > 0:\n",
        "                episodic_returns, scores = evaluate(q_network, make_env, args.env_id, args.eval_episodes, f\"{run_name}-eval-{eval_idx}\", args.QNetwork, args.seed, device, args.eval_epsilon, capture_video=False)\n",
        "                # mean_return = np.mean(episodic_returns)\n",
        "                # print(f\"{prefix}evaluation_{eval_idx}_mean_return={mean_return}\")\n",
        "                # trial.report(mean_return, global_step)\n",
        "                mean_score = np.mean(scores)\n",
        "                print(f\"{prefix}evaluation_{eval_idx}_mean_score={mean_score}\")\n",
        "                trial.report(mean_score, global_step)\n",
        "                eval_idx += 1\n",
        "\n",
        "                # Check if the trial should be pruned\n",
        "                if trial and trial.should_prune():\n",
        "                    raise optuna.exceptions.TrialPruned()\n",
        "                break\n",
        "\n",
        "        # TRY NOT TO MODIFY: save data to reply buffer; handle `terminal_observation`\n",
        "        real_next_obs = next_obs.copy()\n",
        "        #for idx, d in enumerate(dones):\n",
        "            #if d:\n",
        "                #real_next_obs[idx] = infos[idx][\"terminal_observation\"]  //Tetris environment does not set a terminal observation.\n",
        "        rb.add(obs, real_next_obs, actions, rewards, dones, infos)\n",
        "\n",
        "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
        "        obs = next_obs\n",
        "\n",
        "        # ALGO LOGIC: training.\n",
        "        if global_step > args.learning_starts:\n",
        "            if global_step % args.train_frequency == 0:\n",
        "                data = rb.sample(args.batch_size)\n",
        "                with torch.no_grad():\n",
        "                    target_max, _ = target_network(data.next_observations).max(dim=1)\n",
        "                    td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
        "                old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
        "                loss = F.mse_loss(td_target, old_val)\n",
        "\n",
        "                if global_step % 100 == 0:\n",
        "                    writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
        "                    writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
        "                    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "                # optimize the model\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # update target network\n",
        "            if global_step % args.target_network_frequency == 0:\n",
        "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
        "                    target_network_param.data.copy_(\n",
        "                        args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
        "                    )\n",
        "\n",
        "            if global_step % args.backup_frequency == 0:\n",
        "                model_backup_path = f\"runs/{run_name}/{args.exp_name}.backup\"\n",
        "                torch.save(q_network.state_dict(), model_backup_path)\n",
        "\n",
        "    if args.save_model:\n",
        "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
        "        torch.save(q_network.state_dict(), model_path)\n",
        "        print(f\"{prefix}model saved to {model_path}\")\n",
        "\n",
        "        episodic_returns, scores = evaluate(\n",
        "            model_path,\n",
        "            make_env,\n",
        "            args.env_id,\n",
        "            args.eval_episodes,\n",
        "            run_name=f\"{run_name}-eval-{eval_idx}\",\n",
        "            Model=args.QNetwork,\n",
        "            seed=args.seed,\n",
        "            device=device,\n",
        "            epsilon=args.eval_epsilon,\n",
        "            capture_video=args.capture_video\n",
        "        )\n",
        "        # if trial:\n",
        "        #   mean_return = np.mean(episodic_returns)\n",
        "        #   print(f\"{prefix}evaluation_{eval_idx}_mean_return={mean_return}\")\n",
        "        #   trial.set_user_attr(\"mean_return\", float(mean_return))\n",
        "        # for idx, episodic_return in enumerate(episodic_returns):\n",
        "        #     writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
        "        if trial:\n",
        "            mean_score = np.mean(scores)\n",
        "            print(f\"{prefix}evaluation_{eval_idx}_mean_score={mean_score}\")\n",
        "            trial.set_user_attr(\"mean_score\", float(mean_score))\n",
        "        for idx, score in enumerate(scores):\n",
        "            writer.add_scalar(\"eval/final_score\", score, idx)\n",
        "\n",
        "        if args.upload_model:\n",
        "            pass\n",
        "            from cleanrl_utils.huggingface import push_to_hub\n",
        "\n",
        "            repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
        "            repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
        "            push_to_hub(args, episodic_returns, repo_id, \"DQN\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
        "  except:\n",
        "    raise\n",
        "  finally:\n",
        "    envs.close()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZCEBIoO8pd9D"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id, seed, idx, capture_video, run_name, video_freq=100):\n",
        "    def thunk():\n",
        "        env = gym_tetris.make(env_id)\n",
        "        env = JoypadSpace(env, MOVEMENT)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        if capture_video:\n",
        "            if idx == 0:\n",
        "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\", episode_trigger=lambda ep_num: ep_num % video_freq == 0)\n",
        "                \n",
        "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "        env = gym.wrappers.GrayScaleObservation(env)\n",
        "        env = gym.wrappers.FrameStack(env, 1)\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n2UgLd8i0n2"
      },
      "source": [
        "### Cleanrl utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lUe8keIDizOx"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Union\n",
        "\n",
        "def evaluate(\n",
        "    model_input: Union[str, torch.nn.Module],\n",
        "    make_env: Callable,\n",
        "    env_id: str,\n",
        "    eval_episodes: int,\n",
        "    run_name: str,\n",
        "    Model: torch.nn.Module,\n",
        "    seed: int,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        "    epsilon: float = 0.05,\n",
        "    capture_video: bool = True,\n",
        "    video_frequency: int = 1,\n",
        "    env_cnt: int = 1\n",
        "):\n",
        "    env_fns = [make_env(env_id, seed, i, capture_video, run_name, video_frequency) for i in range(env_cnt)]\n",
        "    envs = gym.vector.SyncVectorEnv(env_fns)\n",
        "    if isinstance(model_input, str):\n",
        "        model = Model(envs.single_action_space.n).to(device)\n",
        "        model.load_state_dict(torch.load(model_input, map_location=device))\n",
        "    elif isinstance(model_input, torch.nn.Module):\n",
        "        model = model_input\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_input. It should be either a path (str) or a model instance (torch.nn.Module).\")\n",
        "    model.eval()\n",
        "\n",
        "    obs = envs.reset()\n",
        "    episodic_returns = []\n",
        "    scores = []\n",
        "    while len(episodic_returns) < eval_episodes:\n",
        "        if random.random() < epsilon:\n",
        "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "        else:\n",
        "            q_values = model(torch.Tensor(obs).to(device))\n",
        "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "        next_obs, _, _, infos = envs.step(actions)\n",
        "        for info in infos:\n",
        "            if \"episode\" in info.keys():\n",
        "                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}, score={info['score']}\")\n",
        "                episodic_returns += [info[\"episode\"][\"r\"]]\n",
        "                scores += [info[\"score\"]]\n",
        "        obs = next_obs\n",
        "    envs.close()\n",
        "    return episodic_returns, scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhHGXzJGi-yI"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M26LSBq7UOv2"
      },
      "outputs": [],
      "source": [
        "# # Short Run\n",
        "# class Args:\n",
        "#     def __init__(self):\n",
        "#         self.env_id = \"TetrisA-v0\"\n",
        "#         self.exp_name = \"dqn\"\n",
        "#         self.seed = 1\n",
        "#         self.torch_deterministic = True\n",
        "#         self.cuda = True\n",
        "#         self.mps = False\n",
        "#         self.track = False\n",
        "#         self.wandb_project_name = \"cleanRL\"\n",
        "#         self.wandb_entity = None\n",
        "#         self.capture_video = True\n",
        "#         self.save_model = True\n",
        "#         self.upload_model = False\n",
        "#         self.hf_entity = \"\"\n",
        "#         self.total_timesteps = 20000\n",
        "#         self.video_frequency = 100\n",
        "#         self.env_cnt = 2\n",
        "\n",
        "#         # evaluations\n",
        "#         self.total_evaluations = 2\n",
        "#         self.eval_episodes = 4\n",
        "#         self.eval_frequency = int(self.total_timesteps / self.total_evaluations)\n",
        "#         self.eval_epsilon = 0.05\n",
        "\n",
        "#         # Trainable hyperparameters\n",
        "#         self.QNetwork = None\n",
        "#         self.learning_rate = None\n",
        "#         self.buffer_size = None\n",
        "#         self.gamma = None\n",
        "#         self.tau = None\n",
        "#         self.target_network_frequency = None\n",
        "#         self.batch_size = None\n",
        "#         self.start_e = None\n",
        "#         self.end_e = None\n",
        "#         self.exploration_fraction = None\n",
        "#         self.learning_starts = None\n",
        "#         self.train_frequency = None\n",
        "\n",
        "# args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a74aYTyfXD3A"
      },
      "outputs": [],
      "source": [
        "# # Weeklong args\n",
        "# class Args:\n",
        "#     def __init__(self):\n",
        "#         self.exp_name = \"Tetris_DQN\"\n",
        "#         self.seed = 1\n",
        "#         self.torch_deterministic = True\n",
        "#         self.cuda = False\n",
        "#         self.mps = True\n",
        "#         self.track = False\n",
        "#         self.wandb_project_name = \"cleanRL\"\n",
        "#         self.wandb_entity = None\n",
        "#         self.capture_video = True\n",
        "#         self.save_model = True\n",
        "#         self.upload_model = False\n",
        "#         self.hf_entity = \"\"\n",
        "#         self.env_id = \"TetrisA-v4\"\n",
        "#         self.total_timesteps = 100000\n",
        "#         self.learning_rate = 1e-4\n",
        "#         self.buffer_size = 10000\n",
        "#         self.gamma = 0.99\n",
        "#         self.tau = 1.0\n",
        "#         self.target_network_frequency = 1000\n",
        "#         self.batch_size = 32\n",
        "#         self.start_e = 1\n",
        "#         self.end_e = 0.01\n",
        "#         self.exploration_fraction = 0.30\n",
        "#         self.learning_starts = 5000\n",
        "#         self.train_frequency = 4\n",
        "#         self.video_frequency = 10\n",
        "#         self.env_cnt = 1\n",
        "#         self.QNetwork = get_model_class(\"tiny\")\n",
        "\n",
        "#         # evaluations\n",
        "#         self.total_evaluations = 2\n",
        "#         self.eval_episodes = 4\n",
        "#         self.eval_frequency = int(self.total_timesteps / self.total_evaluations)\n",
        "#         self.eval_epsilon = 0.05\n",
        "\n",
        "# args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_lId5hUjDm",
        "outputId": "c886e297-b83f-465d-bba7-06f47371cb71"
      },
      "outputs": [],
      "source": [
        "# train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feANY7_C43d9"
      },
      "source": [
        "## Optuna Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nOrR3bdr6SWT"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gy8Tuul96eq2"
      },
      "outputs": [],
      "source": [
        "N_TRIALS = 100  # Maximum number of trials\n",
        "N_JOBS = 1 # Number of jobs to run in parallel\n",
        "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
        "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
        "# N_TIMESTEPS = int(2e4)  # Training budget\n",
        "# EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
        "# N_EVAL_ENVS = 5\n",
        "# N_EVAL_EPISODES = 10\n",
        "TIMEOUT = int(60 * 15)  # 15 minutes\n",
        "\n",
        "#ENV_ID = \"TetrisA-v0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optuna Args\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.env_id = \"TetrisA-v5\"\n",
        "        self.exp_name = \"dqn\"\n",
        "        self.seed = 1\n",
        "        self.torch_deterministic = True\n",
        "        self.cuda = True\n",
        "        self.mps = False\n",
        "        self.track = False\n",
        "        self.wandb_project_name = \"cleanRL\"\n",
        "        self.wandb_entity = None\n",
        "        self.capture_video = True\n",
        "        self.save_model = True\n",
        "        self.upload_model = False\n",
        "        self.hf_entity = \"\"\n",
        "        self.total_timesteps = 500000\n",
        "        self.video_frequency = 10\n",
        "        self.env_cnt = 1\n",
        "        self.backup_frequency = 10000\n",
        "\n",
        "        # evaluations\n",
        "        self.total_evaluations = 2\n",
        "        self.eval_episodes = 4\n",
        "        self.eval_frequency = int(self.total_timesteps / self.total_evaluations)\n",
        "        self.eval_epsilon = 0.05\n",
        "\n",
        "        # Trainable hyperparameters\n",
        "        self.QNetwork = None\n",
        "        self.learning_rate = None\n",
        "        self.buffer_size = None\n",
        "        self.gamma = None\n",
        "        self.tau = None\n",
        "        self.target_network_frequency = None\n",
        "        self.batch_size = None\n",
        "        self.start_e = None\n",
        "        self.end_e = None\n",
        "        self.exploration_fraction = None\n",
        "        self.learning_starts = None\n",
        "        self.train_frequency = None\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3eVBzaoSuj2q"
      },
      "outputs": [],
      "source": [
        "def sample_params(trial: optuna.Trial) -> dict:\n",
        "    params = {\n",
        "        \"QNetwork\": get_model_class(trial.suggest_categorical(\"model_size\", [\"tiny\", \"small\", \"original\", \"large\", \"atari\"])),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
        "        \"buffer_size\": trial.suggest_int(\"buffer_size\", 1000, 5000),\n",
        "        \"gamma\": 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True),\n",
        "        \"tau\": 1.0 - trial.suggest_float(\"tau\", 0.00001, 0.1, log=True),\n",
        "        \"target_network_frequency\": trial.suggest_int(\"target_network_frequency\", 50, 500),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
        "        \"start_e\": trial.suggest_float(\"start_e\", 0.8, 1.0),\n",
        "        \"end_e\": trial.suggest_float(\"end_e\", 0.001, 0.1),\n",
        "        \"exploration_fraction\": trial.suggest_float(\"exploration_fraction\", 0.1, 0.9),\n",
        "        \"learning_starts\": trial.suggest_int(\"learning_starts\", 1000, 5000),\n",
        "        \"train_frequency\": trial.suggest_int(\"train_frequency\", 1, 10)\n",
        "    }\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n2qhmpKfpc3p"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.Trial):\n",
        "    args = Args()\n",
        "    hyperparameters = sample_params(trial)\n",
        "    for key, value in hyperparameters.items():\n",
        "        setattr(args, key, value)\n",
        "\n",
        "    nan_encountered = False\n",
        "    try:\n",
        "      train(args, trial)\n",
        "    except AssertionError as e:\n",
        "      # Sometimes, random hyperparams can generate NaN\n",
        "      print(e)\n",
        "      nan_encountered = True\n",
        "    except optuna.exceptions.TrialPruned:\n",
        "      raise\n",
        "\n",
        "    # Tell the optimizer that the trial failed\n",
        "    if nan_encountered:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    # return trial.user_attrs[\"mean_return\"]\n",
        "    return trial.user_attrs[\"mean_score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fG2JPF25tjlR"
      },
      "outputs": [],
      "source": [
        "study_name = f\"{args.env_id}-{args.exp_name}-test\"\n",
        "study_num = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-04-08 12:11:39,121]\u001b[0m A new study created in RDB with name: TetrisA-v5-dqn-test-1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Set pytorch num threads to 1 for faster training\n",
        "torch.set_num_threads(1)\n",
        "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
        "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
        "# Do not prune before 1/3 of the max budget is used\n",
        "pruner = MedianPruner(\n",
        "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=args.total_timesteps // 3\n",
        ")\n",
        "# Create the study and start the hyperparameter optimization\n",
        "study = optuna.create_study(study_name=f\"{study_name}-{study_num}\", storage=\"sqlite:///db.sqlite3\", sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
        "study_num += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "06doOr1-oreV",
        "outputId": "521404ab-fec8-45fe-9b72-48dd862c3720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trial 0: global_step=7414, episodic_return=-1339.0, score=0\n"
          ]
        }
      ],
      "source": [
        "# study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
        "study.optimize(objective)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"  User attrs:\")\n",
        "for key, value in trial.user_attrs.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Write report\n",
        "study.trials_dataframe().to_csv(f\"study_results_dqn_{study_name}_{study_num}.csv\")\n",
        "\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig2 = plot_param_importances(study)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q-ZggO-qtbL"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nmdj4k4rg_k",
        "outputId": "e3a77d57-8695-4cec-e510-b14164cc0eb1"
      },
      "outputs": [],
      "source": [
        "# !optuna-dashboard sqlite:///db.sqlite3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "W1KbrN9Li5i8",
        "ORRWWz8dIqaH"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tetrisenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a7f40d72114c62e91de8d79ab05a4f10d2c48422b550572d6072f074507511d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
