{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "74u8GL2dfD2F"
      },
      "outputs": [],
      "source": [
        "!rm -rf runs videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KbrN9Li5i8"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSm_UBPsLzq_",
        "outputId": "39d8ba22-4808-4519-d1ce-388cb794e306"
      },
      "outputs": [],
      "source": [
        "!pip install gym-tetris\n",
        "!pip install tensorboardX\n",
        "!pip install pyglet==1.5.1\n",
        "!pip install torchsummary\n",
        "!pip install optuna\n",
        "!pip install optuna-dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbigCoubxceQ",
        "outputId": "da980382-8858-44f8-e876-39872e99ac1f"
      },
      "outputs": [],
      "source": [
        "!pip install setuptools==65.5.1\n",
        "!pip install gym==0.21.0\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc0SC4CrMXSk",
        "outputId": "72a96155-d3d3-4f64-e143-6913e97a2d07"
      },
      "outputs": [],
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1NHYcVNMQaW",
        "outputId": "48e3c777-fc26-4b63-eba1-6e11e47aa30c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x103d57ad0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1024, 768))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORRWWz8dIqaH"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAZVhbsLMHdd",
        "outputId": "38351819-5de4-4496-c0d9-f9fc27a5ce70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action space:  ()\n",
            "(240, 256, 3)\n",
            "{'current_piece': 'Jd', 'number_of_lines': 0, 'score': 0, 'next_piece': 'Ih', 'statistics': {'T': 0, 'J': 1, 'Z': 0, 'O': 0, 'S': 0, 'L': 0, 'I': 0}, 'board_height': 0}\n"
          ]
        }
      ],
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_tetris\n",
        "from gym_tetris.actions import MOVEMENT\n",
        "\n",
        "env = gym_tetris.make('TetrisA-v0')\n",
        "env = JoypadSpace(env, MOVEMENT)\n",
        "\n",
        "print(\"action space: \", env.action_space.shape)\n",
        "#print(\"observation space: \", env.observation_shape.shape)\n",
        "\n",
        "done = True\n",
        "for step in range(1):\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    print(state.shape)\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    print(info)\n",
        "    env.render()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQI2Rw4wHCP2",
        "outputId": "e46831e0-cca4-47f2-f664-33c902d5b408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action space:  None\n",
            "(1, 4, 84, 84)\n",
            "[{'lives': 3, 'episode_frame_number': 3, 'frame_number': 3}]\n"
          ]
        }
      ],
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_tetris\n",
        "from gym_tetris.actions import MOVEMENT\n",
        "\n",
        "envs = gym.vector.SyncVectorEnv([make_env(env_id=\"SpaceInvadersNoFrameskip-v4\", seed=1, idx=0, capture_video=0, run_name=\"test\")])\n",
        "\n",
        "print(\"action space: \", envs.action_space.shape)\n",
        "#print(\"observation space: \", env.observation_shape.shape)\n",
        "\n",
        "done = True\n",
        "for step in range(1):\n",
        "    if done:\n",
        "        state = envs.reset()\n",
        "    print(state.shape)\n",
        "    state, reward, done, info = envs.step(envs.action_space.sample())\n",
        "    print(info)\n",
        "    #envs.render()\n",
        "\n",
        "envs.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB-OD5xIv04f"
      },
      "outputs": [],
      "source": [
        "envs = gym.vector.SyncVectorEnv([make_env(env_id=\"TetrisA-v0\", seed=1, idx=0, capture_video=0, run_name=\"test\")])\n",
        "print(\"action space: \", envs.action_space)\n",
        "print(\"observation space: \", envs.observation_space)\n",
        "print(\"single action space: \", envs.single_action_space)\n",
        "print(\"single observation space: \", envs.single_observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKXi7x9lw2oW",
        "outputId": "91ff2ee6-6e92-42dc-b7ee-27f90db5d8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action space:  Tuple(Discrete(6))\n",
            "observation space:  (1, 4, 84, 84)\n",
            "single action space:  Discrete(6)\n"
          ]
        }
      ],
      "source": [
        "envs = gym.vector.SyncVectorEnv([make_env(env_id=\"SpaceInvadersNoFrameskip-v4\", seed=1, idx=0, capture_video=0, run_name=\"test\")])\n",
        "print(\"action space: \", envs.action_space)\n",
        "print(\"observation space: \", envs.observation_space.shape)\n",
        "print(\"single action space: \", envs.single_action_space)\n",
        "#print(\"single observation space: \", envs.single_observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "tEcVDe1dNMp-",
        "outputId": "2f3aabde-2c30-4ba5-ee16-be359cac1ece"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=256x240 at 0x7F4AB7FFFD30>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-842cc5644eca>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PNG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2241\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import io\n",
        "import time\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_tetris\n",
        "from gym_tetris.actions import MOVEMENT\n",
        "\n",
        "env = gym_tetris.make('TetrisA-v0')\n",
        "env = JoypadSpace(env, MOVEMENT)\n",
        "\n",
        "done = True\n",
        "for step in range(5000):\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "\n",
        "    # Capture the current game frame\n",
        "    buffer = io.BytesIO()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    Image.fromarray(img).save(buffer, 'PNG')\n",
        "    buffer.seek(0)\n",
        "\n",
        "    # Display the frame using IPython.display\n",
        "    IPython.display.clear_output(wait=True)\n",
        "    IPython.display.display(Image.open(buffer))\n",
        "\n",
        "    # Sleep for a short duration (optional, to control the frame rate)\n",
        "    #time.sleep(0.1)\n",
        "\n",
        "IPython.display.clear_output(wait=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kImBr6NFjKIY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TidRAkGEU0Wl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    ClipRewardEnv,\n",
        "    EpisodicLifeEnv,\n",
        "    FireResetEnv,\n",
        "    MaxAndSkipEnv,\n",
        "    NoopResetEnv,\n",
        ")\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Get the absolute path to the parent directory of gym-tetris\n",
        "gym_tetris_parent_path = os.path.abspath(os.path.join('..', 'gym-tetris'))\n",
        "\n",
        "# Append the path to the sys.path\n",
        "sys.path.append(gym_tetris_parent_path)\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_tetris\n",
        "from gym_tetris.actions import MOVEMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H-rvKojYAuK3"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, actions_num, arch_fn):\n",
        "        super().__init__()\n",
        "        self.network = arch_fn(actions_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x / 255.0)\n",
        "\n",
        "def original(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, 8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, 4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, 3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, actions_num),\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def tiny(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        nn.Conv2d(1, 8, 3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(8, 16, 5, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 5, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(2048, 128),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, actions_num),\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def small(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        # (1, 84, 84)\n",
        "        nn.Conv2d(1, 64, 7, stride=3),\n",
        "        nn.ReLU(),\n",
        "        # (64, 26, 26)\n",
        "        nn.Conv2d(64, 128, 5),\n",
        "        nn.ReLU(),\n",
        "        # (128, 22, 22)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (128, 11, 11)\n",
        "        nn.Conv2d(128, 128, 3),\n",
        "        nn.ReLU(),\n",
        "        # (128, 8, 8)\n",
        "        nn.Conv2d(128, 256, 3),\n",
        "        nn.ReLU(),\n",
        "        # (256, 6, 6)\n",
        "        nn.Conv2d(256, 256, 3),\n",
        "        nn.ReLU(),\n",
        "        # (256, 4, 4)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (256, 2, 2)\n",
        "        nn.Flatten(),\n",
        "        # 1024\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(256, actions_num)\n",
        "        # 12 possible actions\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def large(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        # (1, 84, 84)\n",
        "        nn.Conv2d(1, 64, 7, stride=3),\n",
        "        nn.ReLU(),\n",
        "        # (128, 26, 26)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (128, 13, 13)\n",
        "        nn.Conv2d(64, 128, 4),\n",
        "        nn.ReLU(),\n",
        "        # (128, 10, 10)\n",
        "        nn.Conv2d(128, 256, 3),\n",
        "        nn.ReLU(),\n",
        "        # (256, 8, 8)\n",
        "        nn.MaxPool2d(2),\n",
        "        # (256, 4, 4)\n",
        "        nn.Flatten(),\n",
        "        # 4096\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(512, actions_num),\n",
        "        # 12 possible actions\n",
        "    )\n",
        "    return network\n",
        "\n",
        "def atari(actions_num):\n",
        "    network = nn.Sequential(\n",
        "        # (1, 84, 84)\n",
        "        nn.Conv2d(1, 16, 8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        # (16, 20, 20)\n",
        "        nn.Conv2d(16, 32, 4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        # (32, 9, 9)\n",
        "        nn.Flatten(),\n",
        "        # 2592\n",
        "        nn.Linear(2592, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, actions_num),\n",
        "    )\n",
        "    return network\n",
        "\n",
        "\n",
        "\n",
        "def get_model_class(architecture=\"original\"):\n",
        "    if architecture == \"original\":\n",
        "        class OriginalQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, original)\n",
        "        return OriginalQNetwork\n",
        "    elif architecture == \"tiny\":\n",
        "        class TinyQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, tiny)\n",
        "        return TinyQNetwork\n",
        "    elif architecture == \"small\":\n",
        "        class SmallQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, small)\n",
        "        return SmallQNetwork\n",
        "    elif architecture == \"large\":\n",
        "        class LargeQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, large)\n",
        "        return LargeQNetwork\n",
        "    elif architecture == \"atari\":\n",
        "        class AtariQNetwork(QNetwork):\n",
        "            def __init__(self, env):\n",
        "                super().__init__(env, atari)\n",
        "        return AtariQNetwork\n",
        "    else:\n",
        "        print(\"Not a valid architecture\")\n",
        "\n",
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esiYvzSJEQrm"
      },
      "outputs": [],
      "source": [
        "Network = get_model_class(\"atari\")\n",
        "model = Network(12)\n",
        "summary(model, (1, 84, 84), batch_size=16, device=\"cpu\")\n",
        "x = torch.randn(1, 84, 84)\n",
        "out = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KbPbg5G8_sO"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I0yXM_e1TGkf"
      },
      "outputs": [],
      "source": [
        "def train(args, trial=None):\n",
        "  try:\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    prefix = \"\"\n",
        "    if trial:\n",
        "      run_name += f\"_trial_{trial.number}\"\n",
        "      prefix = f\"trial {trial.number}: \"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device_name = \"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\"\n",
        "    device_name = \"mps\" if torch.backends.mps.is_available() and args.mps else device_name\n",
        "    device = torch.device(device_name)\n",
        "\n",
        "    # env setup\n",
        "    env_fns = [make_env(args.env_id, args.seed, i, args.capture_video, run_name, args.video_frequency) for i in range(args.env_cnt)]\n",
        "    envs = gym.vector.SyncVectorEnv(env_fns)\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    q_network = args.QNetwork(envs.single_action_space.n).to(device)\n",
        "    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
        "    target_network = args.QNetwork(envs.single_action_space.n).to(device)\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    #summary(q_network, input_size=(1, 84, 84), batch_size=args.batch_size, device=device_name)\n",
        "\n",
        "    rb = ReplayBuffer(\n",
        "        args.buffer_size,\n",
        "        envs.single_observation_space,\n",
        "        envs.single_action_space,\n",
        "        device,\n",
        "        n_envs=args.env_cnt,\n",
        "        optimize_memory_usage=False,\n",
        "        handle_timeout_termination=True,\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    eval_idx = 1\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    obs = envs.reset()\n",
        "    for global_step in range(args.total_timesteps):\n",
        "        # ALGO LOGIC: put action logic here\n",
        "        epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
        "        if random.random() < epsilon:\n",
        "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "        else:\n",
        "            q_values = q_network(torch.Tensor(obs).to(device))\n",
        "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "\n",
        "        # TRY NOT TO MODIFY: execute the game and log data.\n",
        "        next_obs, rewards, dones, infos = envs.step(actions)\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        for info in infos:\n",
        "            if \"episode\" in info.keys():\n",
        "                print(f\"{prefix}global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
        "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
        "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
        "                writer.add_scalar(\"charts/episodic_score\", info[\"score\"], global_step)\n",
        "                writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
        "\n",
        "            # Evaluate and report the agent periodically\n",
        "            if trial and global_step % args.eval_frequency == 0 and global_step > 0:\n",
        "                episodic_returns = evaluate(q_network, make_env, args.env_id, args.eval_episodes, f\"{run_name}-eval-{eval_idx}\", args.QNetwork, args.seed, device, args.eval_epsilon, capture_video=False)\n",
        "                mean_return = np.mean(episodic_returns)\n",
        "                mean_score = np.mean(episodic_scores)\n",
        "                print(f\"{prefix}evaluation_{eval_idx} mean_return={mean_return} mean_score={mean_score}\")\n",
        "                trial.set_user_attr(\"mean_score\", float(mean_score))\n",
        "                trial.report(mean_return, global_step)\n",
        "                eval_idx += 1\n",
        "\n",
        "                # Check if the trial should be pruned\n",
        "                if trial and trial.should_prune():\n",
        "                    raise optuna.exceptions.TrialPruned()\n",
        "                break\n",
        "\n",
        "        # TRY NOT TO MODIFY: save data to reply buffer; handle `terminal_observation`\n",
        "        real_next_obs = next_obs.copy()\n",
        "        #for idx, d in enumerate(dones):\n",
        "            #if d:\n",
        "                #real_next_obs[idx] = infos[idx][\"terminal_observation\"]  //Tetris environment does not set a terminal observation.\n",
        "        rb.add(obs, real_next_obs, actions, rewards, dones, infos)\n",
        "\n",
        "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
        "        obs = next_obs\n",
        "\n",
        "        # ALGO LOGIC: training.\n",
        "        if global_step > args.learning_starts:\n",
        "            if global_step % args.train_frequency == 0:\n",
        "                data = rb.sample(args.batch_size)\n",
        "                with torch.no_grad():\n",
        "                    target_max, _ = target_network(data.next_observations).max(dim=1)\n",
        "                    td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
        "                old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
        "                loss = F.mse_loss(td_target, old_val)\n",
        "\n",
        "                if global_step % 100 == 0:\n",
        "                    writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
        "                    writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
        "                    # print(\"SPS:\", int(global_step / (time.time() - start_time)))    # steps per second\n",
        "                    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "                # optimize the model\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # update target network\n",
        "            if global_step % args.target_network_frequency == 0:\n",
        "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
        "                    target_network_param.data.copy_(\n",
        "                        args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
        "                    )\n",
        "                    \n",
        "            if args.save_model and global_step % args.backup_frequency == 0:\n",
        "                model_backup_path = f\"runs/{run_name}/{args.exp_name}.backup\"\n",
        "                torch.save(q_network.state_dict(), model_backup_path)\n",
        "\n",
        "    if args.save_model:\n",
        "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
        "        torch.save(q_network.state_dict(), model_path)\n",
        "        print(f\"{prefix}model saved to {model_path}\")\n",
        "\n",
        "        episodic_returns, episodic_scores = evaluate(\n",
        "            model_path,\n",
        "            make_env,\n",
        "            args.env_id,\n",
        "            args.eval_episodes,\n",
        "            run_name=f\"{run_name}-eval-{eval_idx}\",\n",
        "            Model=args.QNetwork,\n",
        "            seed=args.seed,\n",
        "            device=device,\n",
        "            epsilon=args.eval_epsilon,\n",
        "            capture_video=args.capture_video\n",
        "        )\n",
        "        \n",
        "        mean_return = np.mean(episodic_returns)\n",
        "        mean_score = np.mean(episodic_scores)\n",
        "        print(f\"{prefix}evaluation_{eval_idx} mean_return={mean_return} mean_score={mean_score}\")\n",
        "        if trial:\n",
        "            trial.set_user_attr(\"mean_return\", float(mean_return))\n",
        "            trial.set_user_attr(\"mean_score\", float(mean_score))\n",
        "        for idx, episodic_return in enumerate(episodic_returns):\n",
        "            writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
        "\n",
        "        if args.upload_model:\n",
        "            pass\n",
        "            from cleanrl_utils.huggingface import push_to_hub\n",
        "\n",
        "            repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
        "            repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
        "            push_to_hub(args, episodic_returns, repo_id, \"DQN\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
        "  except:\n",
        "    raise\n",
        "  finally:\n",
        "    envs.close()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZCEBIoO8pd9D"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id, seed, idx, capture_video, run_name, video_freq=100):\n",
        "    def thunk():\n",
        "        env = gym_tetris.make(env_id)\n",
        "        env = JoypadSpace(env, MOVEMENT)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        if capture_video:\n",
        "            if idx == 0:\n",
        "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\", episode_trigger=lambda ep_num: ep_num % video_freq == 0)\n",
        "        #env = NoopResetEnv(env, noop_max=30)\n",
        "        #env = MaxAndSkipEnv(env, skip=4)\n",
        "        #env = EpisodicLifeEnv(env)\n",
        "        #env = ClipRewardEnv(env)\n",
        "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "        env = gym.wrappers.GrayScaleObservation(env)\n",
        "        env = gym.wrappers.FrameStack(env, 1)\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n2UgLd8i0n2"
      },
      "source": [
        "### Cleanrl utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lUe8keIDizOx"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Union\n",
        "\n",
        "def evaluate(\n",
        "    model_input: Union[str, torch.nn.Module],\n",
        "    make_env: Callable,\n",
        "    env_id: str,\n",
        "    eval_episodes: int,\n",
        "    run_name: str,\n",
        "    Model: torch.nn.Module,\n",
        "    seed: int,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        "    epsilon: float = 0.05,\n",
        "    capture_video: bool = True,\n",
        "    video_frequency: int = 1,\n",
        "    env_cnt: int = 1\n",
        "):\n",
        "    env_fns = [make_env(env_id, seed, i, capture_video, run_name, video_frequency) for i in range(env_cnt)]\n",
        "    envs = gym.vector.SyncVectorEnv(env_fns)\n",
        "    if isinstance(model_input, str):\n",
        "        model = Model(envs.single_action_space.n).to(device)\n",
        "        model.load_state_dict(torch.load(model_input, map_location=device))\n",
        "    elif isinstance(model_input, torch.nn.Module):\n",
        "        model = model_input\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_input. It should be either a path (str) or a model instance (torch.nn.Module).\")\n",
        "    model.eval()\n",
        "\n",
        "    obs = envs.reset()\n",
        "    episodic_returns = []\n",
        "    episodic_scores = []\n",
        "    while len(episodic_returns) < eval_episodes:\n",
        "        #if random.random() < epsilon:\n",
        "            #actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "        #else:\n",
        "        q_values = model(torch.Tensor(obs).to(device))\n",
        "        actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "        next_obs, _, _, infos = envs.step(actions)\n",
        "        for info in infos:\n",
        "            if \"episode\" in info.keys():\n",
        "                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}\")\n",
        "                episodic_returns += [info[\"episode\"][\"r\"]]\n",
        "                episodic_scores += [info[\"score\"]]\n",
        "        obs = next_obs\n",
        "    envs.close()\n",
        "    return episodic_returns, episodic_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhHGXzJGi-yI"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M26LSBq7UOv2"
      },
      "outputs": [],
      "source": [
        "# # Short Run\n",
        "# class Args:\n",
        "#     def __init__(self):\n",
        "#         self.env_id = \"TetrisA-v0\"\n",
        "#         self.exp_name = \"dqn\"\n",
        "#         self.seed = 1\n",
        "#         self.torch_deterministic = True\n",
        "#         self.cuda = True\n",
        "#         self.mps = False\n",
        "#         self.track = False\n",
        "#         self.wandb_project_name = \"cleanRL\"\n",
        "#         self.wandb_entity = None\n",
        "#         self.capture_video = True\n",
        "#         self.save_model = True\n",
        "#         self.upload_model = False\n",
        "#         self.backup_model = True\n",
        "#         self.backup_frequency = 1000\n",
        "#         self.hf_entity = \"\"\n",
        "#         self.total_timesteps = 20000\n",
        "#         self.video_frequency = 100\n",
        "#         self.env_cnt = 2\n",
        "\n",
        "#         # evaluations\n",
        "#         self.total_evaluations = 2\n",
        "#         self.eval_episodes = 4\n",
        "#         self.eval_frequency = int(self.total_timesteps / self.total_evaluations)\n",
        "#         self.eval_epsilon = 0.05\n",
        "\n",
        "#         # Trainable hyperparameters\n",
        "#         self.QNetwork = None\n",
        "#         self.learning_rate = None\n",
        "#         self.buffer_size = None\n",
        "#         self.gamma = None\n",
        "#         self.tau = None\n",
        "#         self.target_network_frequency = None\n",
        "#         self.batch_size = None\n",
        "#         self.start_e = None\n",
        "#         self.end_e = None\n",
        "#         self.exploration_fraction = None\n",
        "#         self.learning_starts = None\n",
        "#         self.train_frequency = None\n",
        "\n",
        "# args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a74aYTyfXD3A"
      },
      "outputs": [],
      "source": [
        "#Weeklong args\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.exp_name = \"Tetris_DQN\"\n",
        "        self.seed = 1\n",
        "        self.torch_deterministic = True\n",
        "        self.cuda = False\n",
        "        self.mps = True\n",
        "        self.track = False\n",
        "        self.wandb_project_name = \"cleanRL\"\n",
        "        self.wandb_entity = None\n",
        "        self.capture_video = True\n",
        "        self.save_model = True\n",
        "        self.upload_model = False\n",
        "        self.hf_entity = \"\"\n",
        "        self.env_id = \"TetrisA-v6\"\n",
        "        self.total_timesteps = 200000\n",
        "        self.learning_rate = 1e-4\n",
        "        self.buffer_size = 20000\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.999\n",
        "        self.target_network_frequency = 1000\n",
        "        self.batch_size = 32\n",
        "        self.start_e = 1\n",
        "        self.end_e = 0.02\n",
        "        self.exploration_fraction = 0.60\n",
        "        self.learning_starts = 5000\n",
        "        self.train_frequency = 4\n",
        "        self.video_frequency = 10\n",
        "        self.env_cnt = 1\n",
        "        self.QNetwork = get_model_class(\"atari\")\n",
        "        self.backup_model = True\n",
        "        self.backup_frequency = 10000\n",
        "\n",
        "        # evaluations\n",
        "        self.total_evaluations = 2\n",
        "        self.eval_episodes = 4\n",
        "        self.eval_frequency = int(self.total_timesteps / self.total_evaluations)\n",
        "        self.eval_epsilon = 0.05\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_lId5hUjDm",
        "outputId": "c886e297-b83f-465d-bba7-06f47371cb71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global_step=8920, episodic_return=-57.5\n",
            "global_step=18244, episodic_return=-39.0\n",
            "global_step=25769, episodic_return=-155.0\n",
            "global_step=30320, episodic_return=-128.0\n",
            "global_step=35357, episodic_return=-96.5\n",
            "global_step=41744, episodic_return=-133.5\n",
            "global_step=50467, episodic_return=-82.5\n",
            "global_step=57578, episodic_return=-73.0\n",
            "global_step=61255, episodic_return=-20.0\n",
            "global_step=68171, episodic_return=-97.5\n",
            "global_step=74417, episodic_return=-20.5\n",
            "global_step=79902, episodic_return=-19.5\n",
            "global_step=83874, episodic_return=-28.5\n",
            "global_step=85618, episodic_return=-96.5\n",
            "global_step=87886, episodic_return=-47.5\n",
            "global_step=90888, episodic_return=-133.0\n",
            "global_step=93663, episodic_return=-45.5\n",
            "global_step=96821, episodic_return=-23.0\n",
            "global_step=98974, episodic_return=-55.5\n",
            "global_step=100635, episodic_return=-82.0\n",
            "global_step=105281, episodic_return=-34.5\n",
            "global_step=108063, episodic_return=17.0\n",
            "global_step=110474, episodic_return=-58.5\n",
            "global_step=113200, episodic_return=-126.5\n",
            "global_step=113898, episodic_return=-39.0\n",
            "global_step=116216, episodic_return=-89.5\n",
            "global_step=118909, episodic_return=-40.0\n",
            "global_step=121255, episodic_return=-31.0\n",
            "global_step=122772, episodic_return=-65.5\n",
            "global_step=124096, episodic_return=-170.0\n",
            "global_step=126345, episodic_return=-52.5\n",
            "global_step=128954, episodic_return=-63.0\n",
            "global_step=133348, episodic_return=15.0\n",
            "global_step=138440, episodic_return=16.5\n",
            "global_step=142943, episodic_return=23.5\n",
            "global_step=152958, episodic_return=62.0\n",
            "global_step=158733, episodic_return=38.5\n",
            "global_step=163769, episodic_return=-32.5\n",
            "global_step=169439, episodic_return=-18.5\n",
            "global_step=177210, episodic_return=18.5\n",
            "global_step=180044, episodic_return=-24.5\n",
            "global_step=183109, episodic_return=-3.0\n",
            "global_step=187138, episodic_return=43.0\n",
            "global_step=189133, episodic_return=5.0\n",
            "global_step=192873, episodic_return=60.5\n",
            "global_step=196525, episodic_return=7.0\n",
            "model saved to runs/TetrisA-v6__Tetris_DQN__1__1680956279/Tetris_DQN.cleanrl_model\n",
            "eval_episode=0, episodic_return=-224.0\n",
            "eval_episode=1, episodic_return=-197.5\n"
          ]
        }
      ],
      "source": [
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feANY7_C43d9"
      },
      "source": [
        "## Optuna Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOrR3bdr6SWT"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy8Tuul96eq2"
      },
      "outputs": [],
      "source": [
        "N_TRIALS = 100  # Maximum number of trials\n",
        "N_JOBS = 2 # Number of jobs to run in parallel\n",
        "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
        "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
        "# N_TIMESTEPS = int(2e4)  # Training budget\n",
        "# EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
        "# N_EVAL_ENVS = 5\n",
        "# N_EVAL_EPISODES = 10\n",
        "TIMEOUT = int(60 * 15)  # 15 minutes\n",
        "\n",
        "#ENV_ID = \"TetrisA-v0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eVBzaoSuj2q"
      },
      "outputs": [],
      "source": [
        "def sample_params(trial: optuna.Trial) -> dict:\n",
        "    params = {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
        "        \"buffer_size\": trial.suggest_int(\"buffer_size\", 1000, 5000),\n",
        "        \"gamma\": 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True),\n",
        "        \"tau\": 1.0 - trial.suggest_float(\"tau\", 0.00001, 0.1, log=True),\n",
        "        \"target_network_frequency\": trial.suggest_int(\"target_network_frequency\", 50, 500),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
        "        \"start_e\": trial.suggest_float(\"start_e\", 0.8, 1.0),\n",
        "        \"end_e\": trial.suggest_float(\"end_e\", 0.001, 0.1),\n",
        "        \"exploration_fraction\": trial.suggest_float(\"exploration_fraction\", 0.1, 0.9),\n",
        "        \"learning_starts\": trial.suggest_int(\"learning_starts\", 1000, 5000),\n",
        "        \"train_frequency\": trial.suggest_int(\"train_frequency\", 1, 10),\n",
        "        \"QNetwork\": get_model_class(trial.suggest_categorical(\"model_size\", [\"tiny\", \"small\", \"original\", \"large\", \"atari\"]))\n",
        "    }\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2qhmpKfpc3p"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.Trial):\n",
        "    args = Args()\n",
        "    hyperparameters = sample_params(trial)\n",
        "    for key, value in hyperparameters.items():\n",
        "        setattr(args, key, value)\n",
        "\n",
        "    nan_encountered = False\n",
        "    try:\n",
        "      train(args, trial)\n",
        "    except AssertionError as e:\n",
        "      # Sometimes, random hyperparams can generate NaN\n",
        "      print(e)\n",
        "      nan_encountered = True\n",
        "    except optuna.exceptions.TrialPruned:\n",
        "      raise\n",
        "\n",
        "    # Tell the optimizer that the trial failed\n",
        "    if nan_encountered:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    return trial.user_attrs[\"mean_return\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG2JPF25tjlR"
      },
      "outputs": [],
      "source": [
        "study_name = f\"{args.env_id}-{args.exp_name}-test\"\n",
        "study_num = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "06doOr1-oreV",
        "outputId": "521404ab-fec8-45fe-9b72-48dd862c3720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-04-04 18:48:05,096]\u001b[0m A new study created in RDB with name: TetrisA-v0-dqn-test-4\u001b[0m\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634085_trial_1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634085_trial_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [16, 32, 20, 20]           2,080\n",
            "              ReLU-2           [16, 32, 20, 20]               0\n",
            "            Conv2d-3             [16, 64, 9, 9]          32,832\n",
            "              ReLU-4             [16, 64, 9, 9]               0\n",
            "            Conv2d-5             [16, 64, 7, 7]          36,928\n",
            "              ReLU-6             [16, 64, 7, 7]               0\n",
            "           Flatten-7                 [16, 3136]               0\n",
            "           Dropout-8                 [16, 3136]               0\n",
            "            Linear-9                  [16, 512]       1,606,144\n",
            "          Dropout-10                  [16, 512]               0\n",
            "             ReLU-11                  [16, 512]               0\n",
            "           Linear-12                   [16, 12]           6,156\n",
            "================================================================\n",
            "Total params: 1,684,140\n",
            "Trainable params: 1,684,140\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.43\n",
            "Forward/backward pass size (MB): 6.11\n",
            "Params size (MB): 6.42\n",
            "Estimated Total Size (MB): 12.97\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [32, 32, 20, 20]           2,080\n",
            "              ReLU-2           [32, 32, 20, 20]               0\n",
            "            Conv2d-3             [32, 64, 9, 9]          32,832\n",
            "              ReLU-4             [32, 64, 9, 9]               0\n",
            "            Conv2d-5             [32, 64, 7, 7]          36,928\n",
            "              ReLU-6             [32, 64, 7, 7]               0\n",
            "           Flatten-7                 [32, 3136]               0\n",
            "           Dropout-8                 [32, 3136]               0\n",
            "            Linear-9                  [32, 512]       1,606,144\n",
            "          Dropout-10                  [32, 512]               0\n",
            "             ReLU-11                  [32, 512]               0\n",
            "           Linear-12                   [32, 12]           6,156\n",
            "================================================================\n",
            "Total params: 1,684,140\n",
            "Trainable params: 1,684,140\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.86\n",
            "Forward/backward pass size (MB): 12.22\n",
            "Params size (MB): 6.42\n",
            "Estimated Total Size (MB): 19.51\n",
            "----------------------------------------------------------------\n",
            "global_step=4713, episodic_return=8.0\n",
            "global_step=4727, episodic_return=11.0\n",
            "global_step=5695, episodic_return=30.0\n",
            "global_step=5758, episodic_return=48.0\n",
            "global_step=5963, episodic_return=2.0\n",
            "global_step=6650, episodic_return=50.0\n",
            "global_step=6754, episodic_return=43.0\n",
            "global_step=7496, episodic_return=39.0\n",
            "global_step=7655, episodic_return=60.0\n",
            "global_step=8372, episodic_return=53.0\n",
            "global_step=9163, episodic_return=91.0\n",
            "global_step=9864, episodic_return=82.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634085_trial_0-eval-1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634085_trial_1-eval-1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_episode=0, episodic_return=13.0\n",
            "eval_episode=1, episodic_return=13.0\n",
            "eval_episode=0, episodic_return=0.0\n",
            "eval_episode=1, episodic_return=0.0\n",
            "eval_episode=2, episodic_return=2.0\n",
            "eval_episode=3, episodic_return=2.0\n",
            "evaluation_1_mean_return=1.0\n",
            "eval_episode=2, episodic_return=10.0\n",
            "eval_episode=3, episodic_return=10.0\n",
            "evaluation_1_mean_return=11.5\n",
            "global_step=10030, episodic_return=0.0\n",
            "global_step=12051, episodic_return=71.0\n",
            "global_step=12474, episodic_return=34.0\n",
            "global_step=11408, episodic_return=22.0\n",
            "global_step=11602, episodic_return=31.0\n",
            "global_step=12441, episodic_return=23.0\n",
            "global_step=13057, episodic_return=56.0\n",
            "global_step=13537, episodic_return=83.0\n",
            "global_step=15321, episodic_return=75.0\n",
            "global_step=14215, episodic_return=67.0\n",
            "global_step=16320, episodic_return=39.0\n",
            "global_step=15420, episodic_return=89.0\n",
            "global_step=16655, episodic_return=24.0\n",
            "global_step=15561, episodic_return=86.0\n",
            "global_step=16358, episodic_return=53.0\n",
            "global_step=17826, episodic_return=46.0\n",
            "global_step=16628, episodic_return=68.0\n",
            "global_step=17612, episodic_return=67.0\n",
            "global_step=18834, episodic_return=102.0\n",
            "global_step=18082, episodic_return=77.0\n",
            "global_step=19599, episodic_return=53.0\n",
            "global_step=18725, episodic_return=44.0\n",
            "model saved to runs/TetrisA-v0__dqn__1__1680634085_trial_0/dqn.cleanrl_model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634085_trial_0-eval-2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global_step=19382, episodic_return=58.0\n",
            "model saved to runs/TetrisA-v0__dqn__1__1680634085_trial_1/dqn.cleanrl_model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634085_trial_1-eval-2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_episode=0, episodic_return=20.0\n",
            "eval_episode=1, episodic_return=12.0\n",
            "eval_episode=0, episodic_return=82.0\n",
            "eval_episode=1, episodic_return=82.0\n",
            "eval_episode=2, episodic_return=18.0\n",
            "eval_episode=2, episodic_return=47.0\n",
            "eval_episode=3, episodic_return=47.0\n",
            "evaluation_2_mean_return=64.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-04-04 18:58:43,029]\u001b[0m Trial 0 finished with value: 64.5 and parameters: {'learning_rate': 0.0005280306275006966, 'buffer_size': 3200, 'gamma': 0.08173960145792994, 'tau': 0.0070037505605310855, 'target_network_frequency': 394, 'batch_size': 32, 'start_e': 0.9418616805831531, 'end_e': 0.07026631202936463, 'exploration_fraction': 0.2786340498222212, 'learning_starts': 2759, 'train_frequency': 5, 'model_size': 'original'}. Best is trial 0 with value: 64.5.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_episode=3, episodic_return=20.0\n",
            "evaluation_2_mean_return=17.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-04-04 18:58:43,731]\u001b[0m Trial 1 finished with value: 17.5 and parameters: {'learning_rate': 0.0001930906083966729, 'buffer_size': 3694, 'gamma': 0.0005952157816468894, 'tau': 0.0043257038165043565, 'target_network_frequency': 458, 'batch_size': 16, 'start_e': 0.860306969639063, 'end_e': 0.034634846117706126, 'exploration_fraction': 0.6796527795801472, 'learning_starts': 3083, 'train_frequency': 9, 'model_size': 'original'}. Best is trial 0 with value: 64.5.\u001b[0m\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634723_trial_2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [64, 64, 26, 26]           3,200\n",
            "              ReLU-2           [64, 64, 26, 26]               0\n",
            "            Conv2d-3          [64, 128, 22, 22]         204,928\n",
            "              ReLU-4          [64, 128, 22, 22]               0\n",
            "         MaxPool2d-5          [64, 128, 11, 11]               0\n",
            "            Conv2d-6            [64, 128, 9, 9]         147,584\n",
            "              ReLU-7            [64, 128, 9, 9]               0\n",
            "            Conv2d-8            [64, 256, 7, 7]         295,168\n",
            "              ReLU-9            [64, 256, 7, 7]               0\n",
            "           Conv2d-10            [64, 256, 5, 5]         590,080\n",
            "             ReLU-11            [64, 256, 5, 5]               0\n",
            "        MaxPool2d-12            [64, 256, 2, 2]               0\n",
            "          Flatten-13                 [64, 1024]               0\n",
            "          Dropout-14                 [64, 1024]               0\n",
            "           Linear-15                  [64, 256]         262,400\n",
            "             ReLU-16                  [64, 256]               0\n",
            "          Dropout-17                  [64, 256]               0\n",
            "           Linear-18                   [64, 12]           3,084\n",
            "================================================================\n",
            "Total params: 1,506,444\n",
            "Trainable params: 1,506,444\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.72\n",
            "Forward/backward pass size (MB): 140.82\n",
            "Params size (MB): 5.75\n",
            "Estimated Total Size (MB): 148.29\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:41: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: Overwriting existing videos at /content/videos/TetrisA-v0__dqn__1__1680634723_trial_3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [64, 64, 26, 26]           3,200\n",
            "              ReLU-2           [64, 64, 26, 26]               0\n",
            "            Conv2d-3          [64, 128, 22, 22]         204,928\n",
            "              ReLU-4          [64, 128, 22, 22]               0\n",
            "         MaxPool2d-5          [64, 128, 11, 11]               0\n",
            "            Conv2d-6            [64, 128, 9, 9]         147,584\n",
            "              ReLU-7            [64, 128, 9, 9]               0\n",
            "            Conv2d-8            [64, 256, 7, 7]         295,168\n",
            "              ReLU-9            [64, 256, 7, 7]               0\n",
            "           Conv2d-10            [64, 256, 5, 5]         590,080\n",
            "             ReLU-11            [64, 256, 5, 5]               0\n",
            "        MaxPool2d-12            [64, 256, 2, 2]               0\n",
            "          Flatten-13                 [64, 1024]               0\n",
            "          Dropout-14                 [64, 1024]               0\n",
            "           Linear-15                  [64, 256]         262,400\n",
            "             ReLU-16                  [64, 256]               0\n",
            "          Dropout-17                  [64, 256]               0\n",
            "           Linear-18                   [64, 12]           3,084\n",
            "================================================================\n",
            "Total params: 1,506,444\n",
            "Trainable params: 1,506,444\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.72\n",
            "Forward/backward pass size (MB): 140.82\n",
            "Params size (MB): 5.75\n",
            "Estimated Total Size (MB): 148.29\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m[W 2023-04-04 18:59:54,822]\u001b[0m Trial 3 failed with parameters: {'learning_rate': 6.855805285103662e-05, 'buffer_size': 4754, 'gamma': 0.0025302609190156044, 'tau': 0.04331334839644418, 'target_network_frequency': 422, 'batch_size': 64, 'start_e': 0.8422605019439453, 'end_e': 0.006240097722642429, 'exploration_fraction': 0.2677176069418209, 'learning_starts': 4632, 'train_frequency': 8, 'model_size': 'small'} because of the following error: RuntimeError('CUDA error: an illegal memory access was encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-15-944ff10dd6ac>\", line 9, in objective\n",
            "    train(args, trial)\n",
            "  File \"<ipython-input-18-47feb1bc2e82>\", line 66, in train\n",
            "    actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
            "RuntimeError: CUDA error: an illegal memory access was encountered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\u001b[33m[W 2023-04-04 18:59:54,826]\u001b[0m Trial 3 failed with value None.\u001b[0m\n",
            "\u001b[33m[W 2023-04-04 18:59:54,839]\u001b[0m Trial 2 failed with parameters: {'learning_rate': 4.441159485251018e-05, 'buffer_size': 2954, 'gamma': 0.0006381212449541604, 'tau': 0.00013290592688321339, 'target_network_frequency': 257, 'batch_size': 64, 'start_e': 0.8683914143302339, 'end_e': 0.011492954992381413, 'exploration_fraction': 0.46911005272538175, 'learning_starts': 4433, 'train_frequency': 9, 'model_size': 'small'} because of the following error: RuntimeError('CUDA error: an illegal memory access was encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-15-944ff10dd6ac>\", line 9, in objective\n",
            "    train(args, trial)\n",
            "  File \"<ipython-input-18-47feb1bc2e82>\", line 65, in train\n",
            "    q_values = q_network(torch.Tensor(obs).to(device))\n",
            "RuntimeError: CUDA error: an illegal memory access was encountered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\u001b[33m[W 2023-04-04 18:59:54,840]\u001b[0m Trial 2 failed with value None.\u001b[0m\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d7a7ba2fd859>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstudy_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_TRIALS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_JOBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0;31m# Raise if exception occurred in executing the completed futures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     futures.add(\n",
            "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-944ff10dd6ac>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnan_encountered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;31m# Sometimes, random hyperparams can generate NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-47feb1bc2e82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# TRY NOT TO MODIFY: execute the game and log data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "args = Args()\n",
        "\n",
        "# Set pytorch num threads to 1 for faster training\n",
        "torch.set_num_threads(1)\n",
        "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
        "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
        "# Do not prune before 1/3 of the max budget is used\n",
        "pruner = MedianPruner(\n",
        "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=args.total_timesteps // 3\n",
        ")\n",
        "# Create the study and start the hyperparameter optimization\n",
        "study = optuna.create_study(study_name=f\"{study_name}-{study_num}\", storage=\"sqlite:///db.sqlite3\", sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
        "study_num += 1\n",
        "try:\n",
        "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"  User attrs:\")\n",
        "for key, value in trial.user_attrs.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Write report\n",
        "study.trials_dataframe().to_csv(f\"study_results_dqn_{study_name}_{study_num}.csv\")\n",
        "\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig2 = plot_param_importances(study)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q-ZggO-qtbL"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nmdj4k4rg_k",
        "outputId": "e3a77d57-8695-4cec-e510-b14164cc0eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listening on http://127.0.0.1:8080/\n",
            "Hit Ctrl-C to quit.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/optuna-dashboard\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/optuna_dashboard/_cli.py\", line 128, in main\n",
            "    run_wsgiref(app, args.host, args.port, args.quiet)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/optuna_dashboard/_cli.py\", line 38, in run_wsgiref\n",
            "    httpd = make_server(host, port, app, server_class=ThreadedWSGIServer)\n",
            "  File \"/usr/lib/python3.9/wsgiref/simple_server.py\", line 154, in make_server\n",
            "    server = server_class((host, port), handler_class)\n",
            "  File \"/usr/lib/python3.9/socketserver.py\", line 452, in __init__\n",
            "    self.server_bind()\n",
            "  File \"/usr/lib/python3.9/wsgiref/simple_server.py\", line 50, in server_bind\n",
            "    HTTPServer.server_bind(self)\n",
            "  File \"/usr/lib/python3.9/http/server.py\", line 137, in server_bind\n",
            "    socketserver.TCPServer.server_bind(self)\n",
            "  File \"/usr/lib/python3.9/socketserver.py\", line 466, in server_bind\n",
            "    self.socket.bind(self.server_address)\n",
            "OSError: [Errno 98] Address already in use\n"
          ]
        }
      ],
      "source": [
        "!optuna-dashboard sqlite:///db.sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4bx38z9qxTo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "W1KbrN9Li5i8",
        "ORRWWz8dIqaH"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tetrisenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a7f40d72114c62e91de8d79ab05a4f10d2c48422b550572d6072f074507511d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
