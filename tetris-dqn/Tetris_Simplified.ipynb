{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KbrN9Li5i8"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSm_UBPsLzq_",
        "outputId": "39d8ba22-4808-4519-d1ce-388cb794e306"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install pyglet==1.5.1\n",
        "!pip install torchsummary\n",
        "!pip install optuna\n",
        "!pip install optuna-dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbigCoubxceQ",
        "outputId": "da980382-8858-44f8-e876-39872e99ac1f"
      },
      "outputs": [],
      "source": [
        "!pip install torchrl\n",
        "!pip install setuptools==65.5.1\n",
        "!pip install gym==0.21.0\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install lz4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc0SC4CrMXSk",
        "outputId": "72a96155-d3d3-4f64-e143-6913e97a2d07"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install -y xvfb\n",
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kImBr6NFjKIY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1NHYcVNMQaW",
        "outputId": "48e3c777-fc26-4b63-eba1-6e11e47aa30c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9ff45ab610>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1024, 768))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the absolute path to the parent directory of gym-tetris\n",
        "gym_tetris_parent_path = os.path.abspath(os.path.join('..', 'gym-tetris'))\n",
        "\n",
        "# Append the path to the sys.path\n",
        "sys.path.append(gym_tetris_parent_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TidRAkGEU0Wl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.1.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "# import gym\n",
        "from gym import Wrapper, ObservationWrapper\n",
        "from gym.wrappers import RecordEpisodeStatistics, RecordVideo, FrameStack\n",
        "from gym.spaces import Box, Discrete\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# from stable_baselines3.common.buffers import ReplayBuffer\n",
        "\n",
        "from tensordict import TensorDict\n",
        "# from torchrl.data import PrioritizedReplayBuffer, ListStorage\n",
        "from torchrl.data import TensorDictPrioritizedReplayBuffer, LazyMemmapStorage, LazyTensorStorage\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_tetris\n",
        "from gym_tetris.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from torchsummary import summary\n",
        "from collections import deque"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, actions_num, frame_stack=1):\n",
        "        super().__init__()\n",
        "        self.network =  nn.Sequential(\n",
        "            # (frame_stack, 20, 10)\n",
        "            nn.Flatten(),\n",
        "            # 200 x frame_stack\n",
        "            nn.Linear(200*frame_stack, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, actions_num),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convolutional on 20x10\n",
        "# class QNetwork(nn.Module):\n",
        "#     def __init__(self, actions_num, frame_stack=1):\n",
        "#         super().__init__()\n",
        "#         self.network =  nn.Sequential(\n",
        "#             # (frame_stack, 20, 10)\n",
        "#             nn.Conv2d(1, 32, 4, stride=2),\n",
        "#             nn.ReLU(),\n",
        "#             # (32, 9, 4)\n",
        "#             nn.Conv2d(32, 64, 2, stride=1),\n",
        "#             nn.ReLU(),\n",
        "#             # (32, 8, 3)\n",
        "#             nn.Flatten(),\n",
        "#             # 768\n",
        "#             nn.Linear(768, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, actions_num)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.network(x)\n",
        "\n",
        "\n",
        "# def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "#     slope = (end_e - start_e) / duration\n",
        "#     return max(slope * t + start_e, end_e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8KbPbg5G8_sO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FrameSkipEnv(Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(FrameSkipEnv, self).__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            # Only do the action on the first frame (action 0 is always NOOP)\n",
        "            real_action = 0 if (i > 0) else action\n",
        "            obs, reward, done, info = self.env.step(real_action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "GAME_BOX = 47, 95, 209, 176\n",
        "BOARD_SHAPE = 20, 10\n",
        "y_step = (GAME_BOX[2] - GAME_BOX[0]) // BOARD_SHAPE[0]\n",
        "x_step = (GAME_BOX[3] - GAME_BOX[1]) // BOARD_SHAPE[1]\n",
        "\n",
        "# Given an image of the current board, obtain a binary (20x10) representation\n",
        "class BinaryBoard(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(0, 1, BOARD_SHAPE)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        gray = np.mean(obs, axis=-1)\n",
        "        cropped = gray[GAME_BOX[0]+(y_step//2) : GAME_BOX[2] : y_step,\n",
        "                       GAME_BOX[1]+(x_step//2) : GAME_BOX[3] : x_step]\n",
        "        assert cropped.shape == BOARD_SHAPE\n",
        "        cropped[cropped > 1] = 1\n",
        "        return cropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorWrapper(ObservationWrapper):\n",
        "    def __init__(self, env=None, device='cpu'):\n",
        "        super(TensorWrapper, self).__init__(env)\n",
        "        self._device = device\n",
        "    \n",
        "    def observation(self, obs):\n",
        "        ret_obs = np.expand_dims(np.array(obs), axis=0)\n",
        "        ret_obs = torch.Tensor(ret_obs).to(self._device)\n",
        "        return ret_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "INNER_SKIP = 16\n",
        "OUTER_SKIP = 3\n",
        "# Making an environment\n",
        "def get_env(env_id, seed, capture_video, run_name, video_freq=100, frame_stack=4, device='cpu'):\n",
        "    env = gym_tetris.make(env_id)\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    \n",
        "    env = RecordEpisodeStatistics(env)\n",
        "    if capture_video:\n",
        "        env = RecordVideo(env, f\"videos/{run_name}\", episode_trigger=lambda ep_num: ep_num % video_freq == 0)\n",
        "    \n",
        "    env = BinaryBoard(env)\n",
        "    env = FrameSkipEnv(env, skip=INNER_SKIP)\n",
        "    env = FrameStack(env, frame_stack)\n",
        "    env = FrameSkipEnv(env, skip=OUTER_SKIP)\n",
        "    env = TensorWrapper(env, device)\n",
        "\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "FPS = 60 / (INNER_SKIP * OUTER_SKIP)\n",
        "SCALE_UP = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "def evaluate(\n",
        "    model: torch.nn.Module,\n",
        "    env_id: str,\n",
        "    eval_episodes: int,\n",
        "    run_name: str,\n",
        "    seed: int,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        "    capture_video: bool = True,\n",
        "    video_frequency: int = 1,\n",
        "    frame_stack: int = 1\n",
        "):\n",
        "    env = get_env(env_id, seed, capture_video, run_name, video_frequency, frame_stack, device=device) \n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    scores = []\n",
        "    for episode in range(eval_episodes):\n",
        "        if capture_video:\n",
        "            out = cv2.VideoWriter(f'eval_episode{episode}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), FPS, (BOARD_SHAPE[1]*SCALE_UP, BOARD_SHAPE[0]*SCALE_UP), False)\n",
        "        \n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if capture_video:\n",
        "                img = np.array(obs).astype('uint8')\n",
        "                if frame_stack > 1:\n",
        "                    img = img[-1]\n",
        "                out.write(img)\n",
        "\n",
        "            q_values = model(obs)\n",
        "            action = int(torch.argmax(q_values))\n",
        "            obs, _, done, info = env.step(action)\n",
        "        \n",
        "        print(f\"eval_episode={len(scores)}, score={info.get('score')}, episodic_return={info.get('episode')['r']}\")\n",
        "        scores.append(info.get(\"score\"))\n",
        "\n",
        "    env.close()\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_obs(out, obs, frame_stack : int = 1, scale_up : int = 1):\n",
        "    img = np.array(obs.cpu())[0].astype('uint8')*255\n",
        "    if frame_stack > 1:\n",
        "        img = img[-1]\n",
        "    if scale_up > 1:\n",
        "        img = np.repeat(np.repeat(img, scale_up, axis=0), scale_up, axis=1)\n",
        "    out.write(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_scalars(writer, global_step, info, epsilon):\n",
        "    writer.add_scalar(\"charts/episodic_return\", info.get(\"episode\")[\"r\"], global_step)\n",
        "    writer.add_scalar(\"charts/episodic_length\", info.get(\"episode\")[\"l\"], global_step)\n",
        "    writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
        "    writer.add_scalar(\"charts/score\", info.get(\"score\"), global_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single env training without optuna - for simplicity\n",
        "def train(args, start_model_path=None):\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{args.run_id}\"\n",
        "    prefix = \"\"\n",
        "    \n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device_name = \"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\"\n",
        "    device_name = \"mps\" if torch.backends.mps.is_available() and args.mps else device_name\n",
        "    device = torch.device(device_name)\n",
        "\n",
        "    print(\"device_name:\", device_name)\n",
        "\n",
        "    # env setup\n",
        "    env = get_env(args.env_id, args.seed, args.capture_video, run_name, args.video_frequency, args.frame_stack, device=device)\n",
        "    assert isinstance(env.action_space, Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    q_network = QNetwork(env.action_space.n, args.frame_stack).to(device)\n",
        "    if start_model_path is not None:\n",
        "        state_dict = torch.load(start_model_path)\n",
        "        q_network.load_state_dict(state_dict)\n",
        "    \n",
        "    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
        "    target_network = QNetwork(env.action_space.n, args.frame_stack).to(device)\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    summary(q_network, input_size=(args.frame_stack, *BOARD_SHAPE), batch_size=args.batch_size, device=device_name)\n",
        "\n",
        "    rb = TensorDictPrioritizedReplayBuffer(\n",
        "        alpha=args.alpha,\n",
        "        beta=args.beta,\n",
        "        storage=LazyTensorStorage(args.buffer_size, device=device),\n",
        "        batch_size=args.batch_size,\n",
        "        prefetch=args.prefetch\n",
        "    )\n",
        "\n",
        "    obs = env.reset()\n",
        "\n",
        "    # Tracks number of episodes simulated\n",
        "    episode_cnt = 0\n",
        "    # Tracks the number of pieces we have played\n",
        "    piece_count = 0\n",
        "    # Whether we explore (play random moves) or exploit (play according to the model)\n",
        "    explore = True\n",
        "    info = None\n",
        "\n",
        "    # Track the best scoring models\n",
        "    scores = deque(maxlen=args.mean_score_count)\n",
        "    best_mean_score = -1.0\n",
        "\n",
        "    if args.capture_inputs_video:\n",
        "        out = cv2.VideoWriter(f'episode0.mp4', cv2.VideoWriter_fourcc(*'mp4v'), FPS, (BOARD_SHAPE[1]*SCALE_UP, BOARD_SHAPE[0]*SCALE_UP), False)\n",
        "\n",
        "    prev_time = time.time()\n",
        "\n",
        "    for global_step in range(args.total_timesteps):\n",
        "\n",
        "        if global_step > 0 and global_step % 100 == 0:\n",
        "           curr_time = time.time()\n",
        "           writer.add_scalar(\"charts/SPS\", 100 / (curr_time - prev_time), global_step)\n",
        "           prev_time = curr_time\n",
        "        \n",
        "        if args.capture_inputs_video and (episode_cnt % args.video_frequency == 0):\n",
        "            write_obs(out, obs, args.frame_stack, SCALE_UP)\n",
        "        \n",
        "        # If a new piece has been generated, decide wether we will explore or exploit for this piece\n",
        "        if (info is not None) and (piece_count != info.get(\"piece_count\")):\n",
        "            piece_count = info.get(\"piece_count\")\n",
        "            if global_step < args.learning_starts:\n",
        "                epsilon = args.start_e\n",
        "            else:\n",
        "                duration = args.exploration_fraction * (args.total_timesteps - args.learning_starts)\n",
        "                epsilon = linear_schedule(args.start_e, args.end_e, duration, global_step - args.learning_starts)\n",
        "            explore = (random.random() < epsilon)\n",
        "\n",
        "        # Find the next action to play\n",
        "        if explore:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            q_values = q_network(obs)\n",
        "            action = int(torch.argmax(q_values))\n",
        "        \n",
        "        # Play a step with the given action\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "        if not done:\n",
        "            # Add observation to replay buffer\n",
        "            data = TensorDict({\"obs\" : obs,\n",
        "                               \"next_obs\" : next_obs,\n",
        "                               \"action\" : [action],\n",
        "                               \"reward\" : [reward],\n",
        "                               \"done\" : [int(done)]},\n",
        "                               batch_size=1, device=device)\n",
        "            rb.add(data)\n",
        "            obs = next_obs\n",
        "        else:\n",
        "            print(f\"Episode {episode_cnt} completed: {prefix}global_step={global_step},\\tepisodic_return={info.get('episode')['r']:.1f},\\tscore={info.get('score')}\")\n",
        "            write_scalars(writer, global_step, info, epsilon)\n",
        "\n",
        "            episode_cnt += 1\n",
        "\n",
        "            scores.append(info.get(\"score\"))\n",
        "            if episode_cnt > args.mean_score_count:\n",
        "                mean_score = sum(scores) / args.mean_score_count\n",
        "                if mean_score > best_mean_score:\n",
        "                    best_mean_score = mean_score\n",
        "                    if global_step > args.learning_starts:\n",
        "                        print(f\"New best mean score: {mean_score}\")\n",
        "                        # Keep a backup of the best scoring model\n",
        "                        best_model_path = f\"runs/{run_name}/{args.exp_name}.best\"\n",
        "                        torch.save(q_network.state_dict(), best_model_path)\n",
        "\n",
        "            if args.capture_inputs_video:\n",
        "                if episode_cnt % args.video_frequency == 0:\n",
        "                    out = cv2.VideoWriter(f'episode{episode_cnt}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), FPS, (BOARD_SHAPE[1]*SCALE_UP, BOARD_SHAPE[0]*SCALE_UP), False)\n",
        "                else:\n",
        "                    out = None\n",
        "            \n",
        "            if episode_cnt % args.reload_env_frequency == 0:\n",
        "                num_reloads = episode_cnt // args.reload_env_frequency\n",
        "                del env\n",
        "                env = get_env(args.env_id, args.seed, args.capture_video, f\"{run_name}_{num_reloads}\", args.video_frequency, args.frame_stack, device=device)\n",
        "            \n",
        "            obs = env.reset()\n",
        "\n",
        "        # Training Logic\n",
        "        if global_step > args.learning_starts:\n",
        "            if global_step % args.train_frequency == 0:\n",
        "                data = rb.sample()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    target_max, _ = target_network(data.get(\"next_obs\")).max(dim=1)\n",
        "                    td_target = data.get(\"reward\").flatten() + args.gamma * target_max * (1 - data.get(\"done\").flatten())\n",
        "                old_val = q_network(data.get(\"obs\")).gather(1, data.get(\"action\")).squeeze()\n",
        "                # loss = F.mse_loss(old_val, td_target)\n",
        "\n",
        "                # Not sure about this part\n",
        "                def weighted_mse_loss(input, target, weight):\n",
        "                    return torch.sum(weight * (input - target) ** 2) \n",
        "\n",
        "                weights = data.get(\"_weight\")\n",
        "                weights /= torch.sum(weights)\n",
        "                loss = weighted_mse_loss(old_val, td_target, weights)\n",
        "\n",
        "                # optimize the model\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Update data priority\n",
        "                td_error = torch.abs(old_val - td_target).unsqueeze(1)\n",
        "                data.set(\"td_error\", td_error)\n",
        "                rb.update_tensordict_priority(data)\n",
        "\n",
        "                # Log training statistics\n",
        "                if global_step % 100 == 0:\n",
        "                    writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
        "                    writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
        "\n",
        "            # update target network\n",
        "            if global_step % args.target_network_frequency == 0:\n",
        "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
        "                    target_network_param.data.copy_(\n",
        "                        args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
        "                    )\n",
        "\n",
        "            if global_step % args.backup_frequency == 0:\n",
        "                best_model_path = f\"runs/{run_name}/{args.exp_name}.backup\"\n",
        "                torch.save(q_network.state_dict(), best_model_path)\n",
        "\n",
        "    if args.save_model:\n",
        "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
        "        torch.save(q_network.state_dict(), model_path)\n",
        "        print(f\"{prefix}model saved to {model_path}\")\n",
        "\n",
        "        scores = evaluate(\n",
        "            q_network,\n",
        "            args.env_id,\n",
        "            args.eval_episodes,\n",
        "            run_name=f\"{run_name}-eval\",\n",
        "            seed=args.seed,\n",
        "            device=device,\n",
        "            capture_video=args.capture_video,\n",
        "            frame_stack=args.frame_stack\n",
        "        )\n",
        "\n",
        "        print(\"Eval Scores:\", scores)\n",
        "        \n",
        "    env.close()\n",
        "    writer.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jhHGXzJGi-yI"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a74aYTyfXD3A"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        # Settings\n",
        "        self.exp_name = \"Tetris_DQN\"\n",
        "        self.run_id = int(time.time())\n",
        "        self.torch_deterministic = True\n",
        "        self.cuda = True\n",
        "        self.mps = False\n",
        "        self.capture_video = True\n",
        "        self.capture_inputs_video = True\n",
        "        self.save_model = True\n",
        "        self.eval_episodes = 1\n",
        "        self.backup_frequency = 10000\n",
        "        self.mean_score_count = 50\n",
        "        self.video_frequency = 50\n",
        "        self.reload_env_frequency = 49\n",
        "        self.prefetch=None\n",
        "\n",
        "        # Hyper-Parameters\n",
        "        self.env_id = \"TetrisA-v5\"\n",
        "        self.frame_stack = 6\n",
        "        self.seed = 2\n",
        "        self.total_timesteps = 500_000\n",
        "        self.learning_rate = 2e-4\n",
        "        self.buffer_size = 20_000\n",
        "        self.learning_starts = 20_000\n",
        "        self.train_frequency = 1\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.999\n",
        "        self.alpha = 0.5\n",
        "        self.beta = 0.5\n",
        "        self.target_network_frequency = 1000\n",
        "        self.batch_size = 32\n",
        "        self.start_e = 1\n",
        "        self.end_e = 0.1\n",
        "        self.exploration_fraction = 0.2\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "74u8GL2dfD2F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'model_input_videos/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r runs/* videos/* model_input_videos/* episode*.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prev_model_path = f\"/mnt/c/Users/User/OneDrive/Documents/GitHub/AtariGameMaster/ImportantMilestones/Squares_only_binary_board/model\"\n",
        "# train(args, start_model_path=prev_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_lId5hUjDm",
        "outputId": "c886e297-b83f-465d-bba7-06f47371cb71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device_name: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                 [32, 1200]               0\n",
            "            Linear-2                 [32, 1024]       1,229,824\n",
            "              ReLU-3                 [32, 1024]               0\n",
            "            Linear-4                  [32, 512]         524,800\n",
            "              ReLU-5                  [32, 512]               0\n",
            "            Linear-6                    [32, 6]           3,078\n",
            "================================================================\n",
            "Total params: 1,757,702\n",
            "Trainable params: 1,757,702\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.15\n",
            "Forward/backward pass size (MB): 1.04\n",
            "Params size (MB): 6.71\n",
            "Estimated Total Size (MB): 7.90\n",
            "----------------------------------------------------------------\n",
            "Episode 0 completed: global_step=244,\tepisodic_return=-55.7,\tscore=0\n",
            "Episode 1 completed: global_step=422,\tepisodic_return=-69.2,\tscore=0\n",
            "Episode 2 completed: global_step=611,\tepisodic_return=-66.9,\tscore=0\n",
            "Episode 3 completed: global_step=844,\tepisodic_return=-58.0,\tscore=0\n",
            "Episode 4 completed: global_step=1020,\tepisodic_return=-69.7,\tscore=0\n",
            "Episode 5 completed: global_step=1231,\tepisodic_return=-62.6,\tscore=0\n",
            "Episode 6 completed: global_step=1484,\tepisodic_return=-53.8,\tscore=0\n",
            "Episode 7 completed: global_step=1698,\tepisodic_return=-61.9,\tscore=0\n",
            "Episode 8 completed: global_step=1890,\tepisodic_return=-66.1,\tscore=0\n",
            "Episode 9 completed: global_step=2083,\tepisodic_return=-66.1,\tscore=0\n",
            "Episode 10 completed: global_step=2273,\tepisodic_return=-66.9,\tscore=0\n",
            "Episode 11 completed: global_step=2521,\tepisodic_return=-54.9,\tscore=0\n",
            "Episode 12 completed: global_step=2746,\tepisodic_return=-59.6,\tscore=0\n",
            "Episode 13 completed: global_step=2929,\tepisodic_return=-68.1,\tscore=0\n",
            "Episode 14 completed: global_step=3155,\tepisodic_return=-59.5,\tscore=0\n",
            "Episode 15 completed: global_step=3410,\tepisodic_return=-53.7,\tscore=0\n",
            "Episode 16 completed: global_step=3631,\tepisodic_return=-60.4,\tscore=0\n",
            "Episode 17 completed: global_step=3819,\tepisodic_return=-67.3,\tscore=0\n",
            "Episode 18 completed: global_step=4024,\tepisodic_return=-63.5,\tscore=0\n",
            "Episode 19 completed: global_step=4296,\tepisodic_return=-50.2,\tscore=0\n",
            "Episode 20 completed: global_step=4470,\tepisodic_return=-70.0,\tscore=0\n",
            "Episode 21 completed: global_step=4716,\tepisodic_return=-55.3,\tscore=0\n",
            "Episode 22 completed: global_step=4957,\tepisodic_return=-56.4,\tscore=0\n",
            "Episode 23 completed: global_step=5120,\tepisodic_return=-72.2,\tscore=0\n",
            "Episode 24 completed: global_step=5335,\tepisodic_return=-61.5,\tscore=0\n",
            "Episode 25 completed: global_step=5469,\tepisodic_return=-78.2,\tscore=0\n",
            "Episode 26 completed: global_step=5708,\tepisodic_return=-56.8,\tscore=0\n",
            "Episode 27 completed: global_step=5868,\tepisodic_return=-72.7,\tscore=0\n",
            "Episode 28 completed: global_step=6077,\tepisodic_return=-63.0,\tscore=0\n",
            "Episode 29 completed: global_step=6233,\tepisodic_return=-73.5,\tscore=0\n",
            "Episode 30 completed: global_step=6478,\tepisodic_return=-55.6,\tscore=0\n",
            "Episode 31 completed: global_step=6725,\tepisodic_return=-55.1,\tscore=0\n",
            "Episode 32 completed: global_step=6948,\tepisodic_return=-60.1,\tscore=0\n",
            "Episode 33 completed: global_step=7201,\tepisodic_return=-54.1,\tscore=0\n",
            "Episode 34 completed: global_step=7379,\tepisodic_return=-69.2,\tscore=0\n",
            "Episode 35 completed: global_step=7502,\tepisodic_return=-80.2,\tscore=0\n",
            "Episode 36 completed: global_step=7655,\tepisodic_return=-74.3,\tscore=0\n",
            "Episode 37 completed: global_step=7917,\tepisodic_return=-52.2,\tscore=0\n",
            "Episode 38 completed: global_step=8111,\tepisodic_return=-65.8,\tscore=0\n",
            "Episode 39 completed: global_step=8276,\tepisodic_return=-71.6,\tscore=0\n",
            "Episode 40 completed: global_step=8472,\tepisodic_return=-65.7,\tscore=0\n",
            "Episode 41 completed: global_step=8692,\tepisodic_return=-60.7,\tscore=0\n",
            "Episode 42 completed: global_step=8944,\tepisodic_return=-54.3,\tscore=0\n",
            "Episode 43 completed: global_step=9159,\tepisodic_return=-61.7,\tscore=0\n",
            "Episode 44 completed: global_step=9372,\tepisodic_return=-62.2,\tscore=0\n",
            "Episode 45 completed: global_step=9560,\tepisodic_return=-67.3,\tscore=0\n",
            "Episode 46 completed: global_step=9754,\tepisodic_return=-65.8,\tscore=0\n",
            "Episode 47 completed: global_step=10012,\tepisodic_return=-53.0,\tscore=0\n",
            "Episode 48 completed: global_step=10170,\tepisodic_return=-73.2,\tscore=0\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "get_env() got multiple values for argument 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args)\n",
            "Cell \u001b[0;32mIn[14], line 132\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, start_model_path)\u001b[0m\n\u001b[1;32m    130\u001b[0m         num_reloads \u001b[39m=\u001b[39m episode_cnt \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m args\u001b[39m.\u001b[39mreload_env_frequency\n\u001b[1;32m    131\u001b[0m         \u001b[39mdel\u001b[39;00m env\n\u001b[0;32m--> 132\u001b[0m         env \u001b[39m=\u001b[39m get_env(args\u001b[39m.\u001b[39;49menv_id, args\u001b[39m.\u001b[39;49mseed, args\u001b[39m.\u001b[39;49mcapture_video, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mrun_name\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mnum_reloads\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, args\u001b[39m.\u001b[39;49mvideo_frequency, args\u001b[39m.\u001b[39;49mframe_stack, num_reloads, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    134\u001b[0m     obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    136\u001b[0m \u001b[39m# Training Logic\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_env() got multiple values for argument 'device'"
          ]
        }
      ],
      "source": [
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{args.run_id}\"\n",
        "model_path = f\"runs/{run_name}/{args.exp_name}.best\"\n",
        "state_dict = torch.load(model_path)\n",
        "net = QNetwork(len(SIMPLE_MOVEMENT), args.frame_stack).to(device)\n",
        "net.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score = evaluate(net, args.env_id, 1, f'{run_name}2-eval', 2, device, True, 1, args.frame_stack)\n",
        "# score = evaluate(net, args.env_id, 1, '', 20, device, True, 1, args.frame_stack, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q-ZggO-qtbL"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "W1KbrN9Li5i8",
        "ORRWWz8dIqaH"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tetrisenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a7f40d72114c62e91de8d79ab05a4f10d2c48422b550572d6072f074507511d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
